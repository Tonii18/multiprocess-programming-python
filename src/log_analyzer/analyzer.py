import os
from datetime import datetime
import json
import threading
import time
import psutil # type: ignore

from multiprocessing import Pool
from src.log_analyzer.extra_functions import messages_analysis
from src.log_analyzer.extra_functions import ips_analysis
from src.log_analyzer.extra_functions import errors_by_hours_analysis

class LogAnalyzer:

    file_name : str
    processes : int
    
    def __init__(self, file_name, processes = 4):
        self.file_name = file_name
        self.processes = processes

        self.lines = []
        self.fragments = []
        self.partial_results = []
        self.results = {}

    # 'Self' is used to indicate the current instance

    # 1. load_file() -> This function is used to load a file and get some information about it

    def load_file(self):
        file_path = self.file_name

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Log file not found: {file_path}")
        
        loaded = 0
        valid_lines = []

        with open(file_path, 'r') as f:
            for row in f:
                line = row.rstrip('\n').strip()
                parts = line.split(';')
                valid_lines.append(line)
                loaded += 1

        self.lines = valid_lines

        return {'path': file_path, 'loaded': loaded}
    
    # 2. split_work() -> It divide the log file content in equals parts

    def split_work(self):
        total_lines = len(self.lines)
        processes = self.processes
        fragment_size = total_lines // processes
        fragments = []

        start = 0

        # This loop calculate de final index of current fragment ('end')

        for i in range(processes):
            end = start + fragment_size
            if i == processes - 1:
                fragments.append(self.lines[start:])
            else:
                fragments.append(self.lines[start:end])
            start = end

        self.fragments = fragments

        return {
            "total_lines": total_lines,
            "fragments": len(fragments),
            "lines_per_fragment": fragment_size,
            "last_fragment_lines": len(fragments[-1])
        }
    
    # 3. analyze_parallel() -> It creates 4 process by using Pool from multiproccesing

    def analyze_parallel(self):
        processes = self.processes
        with Pool(processes) as pool:
            results = pool.map(self.analyze_fragment, self.fragments)
        
        self.partial_results = results
        return self.partial_results

    # 4. combine_results() -> It merge all combined partial results generated by the processes 

    def combine_results(self):
        combined_messages = {}
        combined_ips = {}
        combined_errors_by_hours = {}

        # As we add more metrics, we insert them inside the loop, so that is scalable

        for result in self.partial_results:
            
            # Messages

            for key, value in result["messages"].items():
                combined_messages[key] = combined_messages.get(key, 0) + value

            # IPs

            for ip, count in result["ips"].items():
                combined_ips[ip] = combined_ips.get(ip, 0) + count

            # Errors by hours

            for hour, cnt in result["errors_by_hours"].items():
                combined_errors_by_hours[hour] = combined_errors_by_hours.get(hour, 0) + int(cnt)

        top_10_ips = sorted(combined_ips.items(), key=lambda x: x[1], reverse=True)[:10]

        sorted_errors_by_hours = dict(sorted(
            combined_errors_by_hours.items(),
            key = lambda kv: (kv[0] == "unknown", kv[0])
        ))

        self.results = {
            "total_lines": len(self.lines),
            "messages": combined_messages,
            "top_ips": top_10_ips,
            "errors_by_hour": sorted_errors_by_hours
        }
        
        return self.results

    def show_report(self):
        raise;

    # 5. run_analysis() -> In this function, we run the whole process so that we have less code in main.py

    def run_analysis(self):
        print('\n[1/4] - Cargando fichero...')
        summary = self.load_file()

        print('\n[2/4] - Dividiendo trabajo en fragmentos...')
        split_summary = self.split_work()

        print('\n[3/4] - Análisis paralelo de cada fragmento...')
        self.analyze_parallel()

        print('\n[4/4] - Combinando resultados...')
        results = self.combine_results()

        print('\nAnálisis completado con éxito!\n')
        
        return results

    # 6. Extra function which run every analyzer method and then mix all on analyze_parallel(), easy to extend for adding metrics

    def analyze_fragment(self, fragment):
        message_counts = messages_analysis(fragment)
        ip_counts = ips_analysis(fragment)
        errors_hours_count = errors_by_hours_analysis(fragment)

        return {
            "messages": message_counts,
            "ips": ip_counts,
            "errors_by_hours": errors_hours_count
        }
    
    # 7. This function save the full report on a json file
    # Parameters
    # out_dir: where the folder will be saved
    # filename: file base name, if its null it will be auto generated

    def save_results(self, out_dir: str = "results", filename: str = None):
        os.makedirs(out_dir, exist_ok= True)
        source_base = os.path.splitext(os.path.basename(self.file_name))[0]

        if filename:
            base_name = filename
        else:
            ts = datetime.now().strftime("%Y%m%dT%H%M%S")
            base_name = f"analysis_{source_base}_{ts}"

        out_path = os.path.join(out_dir, f"{base_name}.json")

        payload = {
            "meta": {
                "source_file": self.file_name,
                "created_at": datetime.now().isoformat(),
                "processes": getattr(self, "processes", None),
                "total_lines": getattr(self, "lines", None) and len(self.lines) or self.results.get("total_lines")
            },
            "results": self.results.copy()
        }

        try:
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
        except OSError as e:
            raise OSError(f"Could not write results to {out_path}: {e}") from e
        
        return out_path
